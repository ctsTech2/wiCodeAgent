{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I used a lot of the code cells from this link in order to build this. \n",
    "\n",
    "https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-agent.ipynb#scrollTo=wXi_0ipTvM_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempj\\anaconda3\\envs\\wiCodeAgentEnv\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import langchain_pinecone\n",
    "import pinecone\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "use_serverless = True\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing pinecone client and specify index connection\n",
    "\n",
    "pc_api_key = os.environ.get('PINECONE_API_KEY')\n",
    "pc_index_name = os.environ.get('PINECONE_INDEX_NAME')\n",
    "\n",
    "pc = Pinecone(api_key=pc_api_key)\n",
    "index = pc.Index(pc_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had been getting errors initializing the embedding object. The langchain bot told me how to update it\n",
    "\n",
    "The error message you received is related to the model_name argument in the OpenAIEmbeddings constructor.\n",
    "In recent versions of Langchain, the model_name argument has been moved to the model_kwargs dictionary for consistency with other embedding classes.\n",
    "To fix the error, you can either:\n",
    "\n",
    "• Move the model_name argument to the model_kwargs dictionary:\n",
    "• Use the model argument instead of model_name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model_kwargs={\n",
    "#         \"model_name\": \"text-embedding-ada-002\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# later on, you can add the model_kwargs argument\n",
    "# embeddings.model_kwargs = {\n",
    "#     \"max_length\": 8192,\n",
    "#     \"engine\": \"davinci\",\n",
    "#     \"temperature\": 0.1,\n",
    "#     \"top_p\": 1.0,\n",
    "#     \"n\": 1\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an openAI embedding object\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you're on the right track! If you already have a knowledge base built in Pinecone, you can use Pinecone.from_existing_index(index_name) to connect to your existing Pinecone index and create a Pinecone object.\n",
    "Once you have the Pinecone object, you can create a retriever using Pinecone.as_retriever() method.\n",
    "Here's an example of how you can use the Pinecone object to create a RetrievalQA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our embedding object for queries from a specific index\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone = Pinecone.from_existing_index(\n",
    "    index_name=pc_index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "vectorstore = pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "# We initialize a retriever object from our vectorstore object\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What's the live load for garage floors?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key, #may be redundant if we have api key in .env file\n",
    "    model_name='gpt-4-0125-preview',\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The live load for garage floors is 50 pounds per square foot."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What's the live load for garage floors?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiCodeAgentEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
